{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9p0R9_aMtXa"
   },
   "source": [
    "# **White Noise: Base BERT Fine-Tuning**\n",
    "## **1. Explaining the Problem**\n",
    "\n",
    "`LEGAL-BERT` provides great performance for the Economic / Non-Economic classification task, but it exhibits an underlying bias towards the positive label in the Socio-Cultural / Non-Socio-Cultural classification task. The most flexible and powerful solution to try tackling this issue is to fine-tune the base English BERT transformer, to evaluate whether the Socio-Cultural / Non-Socio-Cultural classification task depends less on legal jargon and more on a broader, everyday language domain.\n",
    "\n",
    "As I specified in LEGAL-BERT's script, my plan was to download the pre-trained `bert-large-uncased`, the extended version of the base English BERT model from the `HuggingFace` library, created by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. It is available at https://huggingface.co/bert-large-uncased. I expected that fine-tuning this transformer for my specific downstream tasks would lead to superior performances for the Socio-Cultural / Non-Socio-Cultural classification problem, and I wanted to try it out with the Economic / Non-Economic categorisation as well.\n",
    "\n",
    "However, when trying to train it within Google CoLab's GPU backend, I crashed against a wall of memory allocation problems, so I opted for the normal version of the base English BERT model, available at https://huggingface.co/bert-base-uncased.\n",
    "\n",
    "This notebook is heavily inspired by Anne Kroon's [version](https://github.com/uvacw/teaching-bdaca/blob/main/modules/machinelearning-text-exercises/transformers_bert_classification.ipynb) of the [the BERT for Humanists Fine-Tuning for a Classification task tutorial](https://colab.research.google.com/drive/19jDqa5D5XfxPU6NQef17BC07xQdRnaKU?usp=sharing), designed by Maria Antoniak, Melanie Walsh, and the [BERT for Humanists](https://melaniewalsh.github.io/BERT-for-Humanists/) Team.\n",
    "\n",
    "These are the steps involved in using BERT and HuggingFace:\n",
    "\n",
    "    1. Split the labelled dataset into training, validation, and testing subsets.\n",
    "    2. Convert the data into a format that BERT can process.\n",
    "    3. Create dataset objects by joining the textual data and labels.\n",
    "    4. Load the pre-trained BERT model.\n",
    "    5. Refine the model by training it on the training set.\n",
    "    6. Use the model to make predictions and assess its performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPXGBC1P6lR2",
    "outputId": "f301d452-8f09-49ad-ba01-0a8ec7cd7916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers==4.28.0 in /usr/local/lib/python3.10/dist-packages (4.28.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n"
     ]
    }
   ],
   "source": [
    "# Installing the \"transformers\" package, in an older version (4.28.0)\n",
    "!pip3 install transformers==4.28.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BO5iLCtlb_Mk"
   },
   "source": [
    "I employ an older version of the `transformers` package because in the recent `4.29.0` release the function or method `PartialState` is not defined, leading to a `NameError` when trying to initialise the training parameters within the `Trainer` object. I must greatly thank the user `amyeroberts` who suggested to downgrade the `transformers` package within the following thread: https://github.com/huggingface/transformers/issues/22816."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XbktqVncNFfS"
   },
   "outputs": [],
   "source": [
    "# General packages\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Packages for data handling and cleaning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Packages for SML and Transformer fine-tuning\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z__WcEQ0Ng87",
    "outputId": "46651557-d0c5-45a6-f9cd-a402b2d204af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Packages for mounting Drive and setting the working directory\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Mounting my Drive on Google Colab\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Setting the working directory\n",
    "os.chdir(r\"/content/drive/My Drive/Colab Notebooks/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMyfGvsG7UOd"
   },
   "source": [
    "## **1. Unpacking the data, and splitting it into Training, Validation, and Test sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OqusMaWU686o"
   },
   "outputs": [],
   "source": [
    "# I start by importing the \"labelled.csv\" data set as a DataFrame object within the CoLab environment.\n",
    "# I crucially specify the \"|\" separator, because employing colons or semi-colons causes conflicts with the summaries' contents.\n",
    "\n",
    "d = pd.read_csv(\"labelled.csv\", sep = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ca7cqu1U7CVM",
    "outputId": "8b227c8a-3af9-4a4e-f522-44fec2df4d4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cf35dd23-8e13-432f-a2bb-2067422531f5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>congress</th>\n",
       "      <th>bill_number</th>\n",
       "      <th>bill_type</th>\n",
       "      <th>text</th>\n",
       "      <th>economic</th>\n",
       "      <th>socio_cultural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>1308</td>\n",
       "      <td>hr</td>\n",
       "      <td>Frank and Jeanne Moore Wild Steelhead Speci...</td>\n",
       "      <td>Non-Economic</td>\n",
       "      <td>Socio-Cultural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115</td>\n",
       "      <td>4105</td>\n",
       "      <td>hr</td>\n",
       "      <td>This bill extends funding through FY2022 for...</td>\n",
       "      <td>Economic</td>\n",
       "      <td>Non-Socio-Cultural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115</td>\n",
       "      <td>3691</td>\n",
       "      <td>s</td>\n",
       "      <td>Expanding Transparency of Information and S...</td>\n",
       "      <td>Non-Economic</td>\n",
       "      <td>Socio-Cultural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111</td>\n",
       "      <td>1994</td>\n",
       "      <td>hr</td>\n",
       "      <td>Citizen Soldier Equality Act of 2009 - Requi...</td>\n",
       "      <td>Economic</td>\n",
       "      <td>Socio-Cultural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111</td>\n",
       "      <td>883</td>\n",
       "      <td>hr</td>\n",
       "      <td>Amends the Internal Revenue Code to repeal, e...</td>\n",
       "      <td>Economic</td>\n",
       "      <td>Socio-Cultural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf35dd23-8e13-432f-a2bb-2067422531f5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cf35dd23-8e13-432f-a2bb-2067422531f5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cf35dd23-8e13-432f-a2bb-2067422531f5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   congress  bill_number bill_type  \\\n",
       "0       115         1308        hr   \n",
       "1       115         4105        hr   \n",
       "2       115         3691         s   \n",
       "3       111         1994        hr   \n",
       "4       111          883        hr   \n",
       "\n",
       "                                                text      economic  \\\n",
       "0     Frank and Jeanne Moore Wild Steelhead Speci...  Non-Economic   \n",
       "1    This bill extends funding through FY2022 for...      Economic   \n",
       "2     Expanding Transparency of Information and S...  Non-Economic   \n",
       "3    Citizen Soldier Equality Act of 2009 - Requi...      Economic   \n",
       "4   Amends the Internal Revenue Code to repeal, e...      Economic   \n",
       "\n",
       "       socio_cultural  \n",
       "0      Socio-Cultural  \n",
       "1  Non-Socio-Cultural  \n",
       "2      Socio-Cultural  \n",
       "3      Socio-Cultural  \n",
       "4      Socio-Cultural  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I check the first few lines of the DataFrame object to assess if the \"read_csv\" command worked smoothly\n",
    "\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KaKXNQoI7E6a",
    "outputId": "f2f11216-f3ed-4137-f895-196e2851b8bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2200, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I check the shape of the DataFrame object to assess if the \"read_csv\" command worked smoothly\n",
    "\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZHxwRmC7Nx6"
   },
   "source": [
    "2200 classified documents, six columns - i.e., the original four columns I retrieved from `api.congress.gov`, plus the two columns that contain the categories I manually annotated. Everything seems perfect! I can now transform the columns where I respectively stored the textual data - i.e. `text` - the economic labels - i.e., `economic` - and the socio-cultural labels - i.e., `socio_cultural` - in three separate lists, which I subsequently split into suitable sets for training, validation, and testing.\n",
    "\n",
    "The following cell's code is inspired by the official documentation for the `tolist()` `pandas` method, available at https://pandas.pydata.org/docs/reference/api/pandas.Series.tolist.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "00kW8FioffR3"
   },
   "outputs": [],
   "source": [
    "# I unpack the columns of interest into three separate lists with the .tolist() pandas method.\n",
    "\n",
    "text = d[\"text\"].tolist() # Textual data\n",
    "economic = d[\"economic\"].tolist() # Economic labels\n",
    "socio_cultural = d[\"socio_cultural\"].tolist() # Socio-cultural labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lqhiJoJC08Zx",
    "outputId": "4542ed34-af7d-4e62-956b-577838336fe1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   Frank and Jeanne Moore Wild Steelhead Special Management Area Designation Act      This bill designates approximately 99,653 acres of Forest Service land in Oregon as the  Frank and Jeanne Moore Wild Steelhead Special Management Area.  ',\n",
       " '  This bill extends funding through FY2022 for the Department of Health and Human Services to award grants to states and certain other entities for demonstration projects that address health-professions workforce needs.  ',\n",
       " '   Expanding Transparency of Information and Safeguarding Toxics (EtO is Toxic) Act of 2018    This bill updates requirements for chemicals that pose an adverse public health risk. Specifically, the bill requires the Environmental Protection Agency (EPA) to publish an updated National Air Toxics Assessment once every two years. The assessment uses emissions data to estimate health risks from toxic air pollutants.    The bill also requires the EPA to use data from its Integrated Risk Information System when conducting rulemaking with respect to chemicals that have been assessed in the system. For chemicals that are found to pose an adverse health risk, the EPA shall identify and do additional review on facilities that are significant sources of the chemical to determine whether the facility poses an adverse public health risk.   Under the bill, chemicals identified as carcinogenic in the system must have a toxic chemical release form completed by the owner or operator of a facility.   The bill requires the Department of Health and Human Services (HHS) to consult with appropriate EPA offices regarding the future schedule of assessments of chemicals to be conducted under the system, the results or existing assessments, and concerns that may merit additional review. HHS must also administer personal exposure tests for chemicals that pose a new adverse public health risk to vulnerable populations, such as children. HHS must establish a Community Outreach Division to communicate risk assessments to affected communities. ',\n",
       " '  Citizen Soldier Equality Act of 2009 - Requires that, in the case of a member of the reserves who is retired or placed on the temporary disability retired list because of an incurred disability for which the member is awarded the Purple Heart, the member shall be credited with the number of years of service that would be counted under the computation of years of service for retired pay for non-regular service. ',\n",
       " ' Amends the Internal Revenue Code to repeal, effective January 1, 2009, the 1993 increase in income taxes on Social Security benefits.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I check the first 5 elements and overall lengths of the three lists to assess whether this data wrangling step went smoothly.\n",
    "\n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FBUu5xh0_NV",
    "outputId": "671918af-d3e0-45b5-cc5d-8162af45c144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The textual data list's total length is 2200.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The textual data list's total length is {len(text)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIvFzUBL1Dsh",
    "outputId": "53f8c9ee-ecec-4885-f7ee-134071489ec9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Non-Economic', 'Economic', 'Non-Economic', 'Economic', 'Economic']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "economic[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTj4TuT21Fxq",
    "outputId": "e15c4ff5-ced8-449b-b3e2-36ff9932e73c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The economic label list's total length is 2200.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The economic label list's total length is {len(economic)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ej09_rvZ1Hri",
    "outputId": "790cff7a-dabe-4bf3-87e9-c247e3170565"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Socio-Cultural',\n",
       " 'Non-Socio-Cultural',\n",
       " 'Socio-Cultural',\n",
       " 'Socio-Cultural',\n",
       " 'Socio-Cultural']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socio_cultural[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-GGfysr1Jf5",
    "outputId": "a0af01f0-a9c2-4ff8-9804-6fd8e2029f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The socio-cultural label list's total length is 2200.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The socio-cultural label list's total length is {len(socio_cultural)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2syrwdpM1Rl0"
   },
   "source": [
    "All data was correctly dumped into separate lists. Now, I proceed to split them into suitable sets for training, validation, and testing with the `sklearn` `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qd8oBVcQ1TTc"
   },
   "outputs": [],
   "source": [
    "# I set a given random seed to make my work reproducible. For the record, the 27th of August is my birthday.\n",
    "my_seed = 27\n",
    "\n",
    "# Running the train vs test split with the standard 80% vs 20% ratio.\n",
    "text_train, text_test, econ_train, econ_test, sc_train, sc_test = train_test_split(\n",
    "    text, economic, socio_cultural, test_size = 0.2, random_state = my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bq652RG31XiI",
    "outputId": "789d2929-f2e9-42eb-d1c8-9e347d096e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text sets have 1760 training instances and 440 testing instances.\n"
     ]
    }
   ],
   "source": [
    "# Checking whether this splitting step went smoothly for the bill summaries...\n",
    "print(f\"The text sets have {len(text_train)} training instances and {len(text_test)} testing instances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQSW2vzK1Z6D",
    "outputId": "ec631d3e-acd7-45d3-9588-f8a3f3b64197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The economic label sets have 1760 training instances and 440 testing instances.\n",
      "The socio-cultural label sets have 1760 training instances and 440 testing instances.\n"
     ]
    }
   ],
   "source": [
    "# ...and for their labels.\n",
    "print(f\"The economic label sets have {len(econ_train)} training instances and {len(econ_test)} testing instances.\")\n",
    "print(f\"The socio-cultural label sets have {len(sc_train)} training instances and {len(sc_test)} testing instances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxIe43If1cCB"
   },
   "source": [
    "I further split the remaining data into training and validation sets. This time, I apply a 75% versus 25% split ratio, saving one fourth of the bill summaries and relative labels for validation purposes, because I want the number of instances for validation and testing to be as close as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IgDh0bz01d5i"
   },
   "outputs": [],
   "source": [
    "# Running the train vs validate split with a 75% vs 25% ratio.\n",
    "text_train, text_valid, econ_train, econ_valid, sc_train, sc_valid = train_test_split(\n",
    "    text_train, econ_train, sc_train, test_size = 0.25, random_state = my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2YAX5671g_v",
    "outputId": "90af484c-38b6-4896-99b5-e27256f1948a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text sets have 1320 training instances and 440 validation instances.\n"
     ]
    }
   ],
   "source": [
    "# Checking whether this splitting step went smoothly for the bill summaries...\n",
    "print(f\"The text sets have {len(text_train)} training instances and {len(text_valid)} validation instances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWnAFi0p1jmc",
    "outputId": "1e3d9fbb-4360-4a8d-8312-b015f546f5f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The economic label sets have 1320 training instances and 440 validation instances.\n",
      "The socio-cultural label sets have 1320 training instances and 440 validation instances.\n"
     ]
    }
   ],
   "source": [
    "# ...and for their labels.\n",
    "print(f\"The economic label sets have {len(econ_train)} training instances and {len(econ_valid)} validation instances.\")\n",
    "print(f\"The socio-cultural label sets have {len(sc_train)} training instances and {len(sc_valid)} validation instances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv4_8wzi2Cig"
   },
   "source": [
    "I now set some variables that will help me to avoid hard-coding some key values, such as the pre-trained BERT model's name, or the directories where I want to save my fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pvRRW80S1snr"
   },
   "outputs": [],
   "source": [
    "# To import base BERT in English, I refer to the \"bert-base-uncased\" transformer.\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# I will rune my code on NVIDIA GPUs using Google CoLab's program management system.\n",
    "device_name = \"cuda\"\n",
    "\n",
    "# I set the maximum number of tokens in each document to be 512, which is the maximum length for BERT models.\n",
    "max_length = 512\n",
    "\n",
    "# I define the directory where I'll save my fine-tuned model for the Economic / Non-Economic classification task.\n",
    "save_directory_econ = \"base_bert_econ\"\n",
    "\n",
    "# I define the directory where I'll save my fine-tuned model for the Socio-Cultural / Non-Socio-Cultural classification task.\n",
    "save_directory_sc = \"base_bert_sc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAxOWLup4Shk"
   },
   "source": [
    "## **2. Encoding data for BERT**\n",
    "\n",
    "To prepare my data set for use with the pre-trained `bert-base-uncased`, I need to encode the texts and labels in a way that the model can understand. Here are the steps I must follow:\n",
    "\n",
    "    1. Convert the labels from strings to integers.\n",
    "    2. Tokenize the texts, which involves breaking them up into individual words, and then convert the words into \"word pieces\" that can be matched with their corresponding embedding vectors.\n",
    "    3. Truncate texts that are longer than 512 tokens, or pad texts that are shorter than 512 tokens with a special padding token.\n",
    "    4. Add special tokens to the beginning and end of each document, including a start token, a separator between sentences, and a padding token as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9ksDM9Gp5poK"
   },
   "outputs": [],
   "source": [
    "# Package for automatic BERT tokenizing\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# I import my automatic BERT tokenizer from the \"bert-base-uncased\" model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziWNvU198M5a"
   },
   "source": [
    "I now generate a mapping of my Economic / Non-Economic, and Socio-Cultural / Non-Socio-Cultural labels to integer keys. I begin by extracting the unique labels from my dataset and creating two dictionaries that associate each label with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Y5Jzy9iI8dv5"
   },
   "outputs": [],
   "source": [
    "# a. Economic / Non-Economic\n",
    "\n",
    "# I create a set called \"unique_labels_econ\" using a set comprehension.\n",
    "# I iterate over each label in the econ_train variable and add it to the set.\n",
    "# The end product of the loop is a set of all the unique labels in econ_train.\n",
    "unique_labels_econ = set(label for label in econ_train)\n",
    "\n",
    "# I now create a dictionary called \"label2id_econ\" using a dictionary comprehension,\n",
    "# by iterating over all labels in the unique_labels_econ set. For each label, I\n",
    "# generate a key-value pair in the dictionary where the key is the label and\n",
    "# the value is its corresponding integer ID, which is defined thanks to the\n",
    "# enumerate() function.\n",
    "label2id_econ = {label: id for id, label in enumerate(unique_labels_econ)}\n",
    "\n",
    "# I finally generate a dictionary called \"id2label_econ\" using another dictionary\n",
    "# comprehension. This time, I iterate over each key-value pair in the newly\n",
    "# created \"label2id_econ\" dictionary, and for each key-value pair, we set a new\n",
    "# key-value pair in the \"id2label_econ\" dictionary, where the key is the integer ID\n",
    "# and the value is the label.\n",
    "id2label_econ = {id: label for label, id in label2id_econ.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfSH9B4g-Ig3",
    "outputId": "a945117f-3b23-4cb7-f3bb-ccd04531273e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Economic', 'Non-Economic'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id_econ.keys() # I check the keys of the \"label2id_econ\" dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arC90MWm-LJE",
    "outputId": "bc1c9e35-6d9f-4ac8-eb4f-27f124a93fbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_econ.keys() # ...and the \"id2label_econ\" dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8NZ-U-AB-YiO"
   },
   "outputs": [],
   "source": [
    "# b. Socio-Cultural / Non-Socio-Cultural\n",
    "\n",
    "# I create a set called \"unique_labels_sc\" using a set comprehension.\n",
    "# I iterate over each label in the sc_train variable and add it to the set.\n",
    "# The end product of the loop is a set of all the unique labels in sc_train.\n",
    "unique_labels_sc = set(label for label in sc_train)\n",
    "\n",
    "# I now create a dictionary called \"label2id_sc\" using a dictionary comprehension,\n",
    "# by iterating over all labels in the unique_labels_sc set. For each label, I\n",
    "# generate a key-value pair in the dictionary where the key is the label and\n",
    "# the value is its corresponding integer ID, which is defined thanks to the\n",
    "# enumerate() function.\n",
    "label2id_sc = {label: id for id, label in enumerate(unique_labels_sc)}\n",
    "\n",
    "# I finally generate a dictionary called \"id2label_sc\" using another dictionary\n",
    "# comprehension. This time, I iterate over each key-value pair in the newly\n",
    "# created \"label2id_sc\" dictionary, and for each key-value pair, we set a new\n",
    "# key-value pair in the \"id2label_sc\" dictionary, where the key is the integer ID\n",
    "# and the value is the label.\n",
    "id2label_sc = {id: label for label, id in label2id_sc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hr_8fCBZ_J-1",
    "outputId": "6331fa96-5b4f-42d3-a0a7-91f9e325f38c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Non-Socio-Cultural', 'Socio-Cultural'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id_sc.keys() # I check the keys of the \"label2id_sc\" dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukoLtNzh_QY0",
    "outputId": "0baf6c88-f486-4221-d3ea-301da88641ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_sc.keys() # ...and the \"id2label_sc\" dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbQxivFg_f8y"
   },
   "source": [
    "Next, I respectively tokenize and encode all bill summaries and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6Cb1pCp-_xxi"
   },
   "outputs": [],
   "source": [
    "# I first encode the bill summaries for training, validation, and testing with the\n",
    "# pre-trained AutoTokenizer from the HuggingFace library. The truncation,\n",
    "# padding, and \"max_length\" parameters are set to ensure that all the tokenized\n",
    "# sequences are of the same length.\n",
    "\n",
    "# 1. The truncation parameter ensures that any sequences longer than \"max_length\" (512)\n",
    "# are truncated to the specified maximum length.\n",
    "\n",
    "# 2. The padding parameter ensures that any sequences shorter than \"max_length\" (512)\n",
    "# are padded with special tokens to the specified maximum length.\n",
    "\n",
    "# 3. The \"max_length\" parameter specifies the maximum length of the tokenized sequences - i.e., 512.\n",
    "\n",
    "train_encodings = tokenizer(text_train, truncation = True, padding = True, max_length = max_length)\n",
    "valid_encodings = tokenizer(text_valid, truncation = True, padding = True, max_length = max_length)\n",
    "test_encodings = tokenizer(text_test, truncation = True, padding = True, max_length = max_length)\n",
    "\n",
    "# I then encode my labels by iterating over the training, validation, and\n",
    "# testing datasets, and mapping each label to its corresponding integer ID,\n",
    "# respectively using the \"label2id_econ\" and \"label2id_sc\" dictionaries.\n",
    "# I store the resulting integer IDs into new \"_encoded\" lists.\n",
    "\n",
    "# a. Economic / Non-Economic\n",
    "econ_train_encoded = [label2id_econ[lab] for lab in econ_train]\n",
    "econ_valid_encoded = [label2id_econ[lab] for lab in econ_valid]\n",
    "econ_test_encoded = [label2id_econ[lab] for lab in econ_test]\n",
    "\n",
    "# b. Socio-Cultural / Non-Socio-Cultural\n",
    "sc_train_encoded = [label2id_sc[lab] for lab in sc_train]\n",
    "sc_valid_encoded = [label2id_sc[lab] for lab in sc_valid]\n",
    "sc_test_encoded = [label2id_sc[lab] for lab in sc_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5ZDwC58Cg3z"
   },
   "source": [
    "After the encoding procedure, I examine the newly created sets to check whether there are any issues. This is a bill summary in the training set after encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "_yVECiI4CvvN",
    "outputId": "b91b954a-d09a-4d1d-ebb2-92c787cc62a9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] protect the homeland from north korean and iranian ballistic missiles act - states the concern of congress over north korean and iranian long - range ballistic missile technology and the spread of such technology . expresses support for ballistic missile protection of u . s . allies and forward deployed forces but also the belief that this should not come at the expense of u . s . homeland protection . directs the secretary of defense to deploy specified numbers of ground - based interceptor ##s in alaska and california and such number in other locations as determined to'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take the first document in the \"train_encodings\" set, and I join its\n",
    "# first 100 tokens by a whitespace to get a sneak peek.\n",
    "\n",
    "\" \".join(train_encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-BRZzyLDLZn"
   },
   "source": [
    "This is a bill summary in the validation set after encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "29XiEWAoDOTr",
    "outputId": "930a839e-44a0-4c32-fcd4-f8b9af4685aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS] continuation of useful resources to states act or courts act this bill extends and otherwise rev ##ises funding for programs related to child welfare . specifically , the bill extends funding through f ##y ##20 ##22 for the promoting safe and stable families program ; extends funding through f ##y ##20 ##22 for , and otherwise rev ##ises , the grant program for improving courts ' handling of foster - care and adoption proceedings ; and provides funding for the temporary assistance for needy families ( tan ##f ) con ##ting ##ency fund for f ##y ##20 ##19 and\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take the first document in the \"valid_encodings\" set, and I join its\n",
    "# first 100 tokens by a whitespace to get a sneak peek.\n",
    "\n",
    "\" \".join(valid_encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NSeWK28F9ac"
   },
   "source": [
    "This is a bill summary in the test set after encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "tqYLnAU2GDX4",
    "outputId": "115afc6b-6bf9-48e0-dbcb-8d706ac2ddc7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] national security commission artificial intelligence act of 2018 this bill establishes , as an independent commission within the executive branch , the national security commission on artificial intelligence to review the advances in artificial intelligence , related machine learning developments , and associated technologies . such commission shall consider the methods and means necessary to advance the development of artificial intelligence , machine learning , and associated technologies by the united states in order to comprehensive ##ly address national security needs , including economic risk , and any other needs of the department of defense or the common defense'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take the first document in the \"test_encodings\" set, and I join its\n",
    "# first 100 tokens by a whitespace to get a sneak peek\n",
    "\n",
    "\" \".join(test_encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNp9-ezaGQB2"
   },
   "source": [
    "These are the Economic / Non Economic, and Socio-Cultural / Non-Socio-Cultural labels for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFk_aL0xGQuO",
    "outputId": "fd8e1002-2ea3-4b34-f887-63d8d02023de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I print the \"econ_train_encoded\" labels in the set format\n",
    "set(econ_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X73gZ_V9GoTI",
    "outputId": "b0b2bbf0-338d-44e9-9c5c-2e13245296e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I print the \"sc_train_encoded\" labels in the set format\n",
    "set(sc_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqk9VxGdGupM"
   },
   "source": [
    "These are the Economic / Non Economic, and Socio-Cultural / Non-Socio-Cultural labels for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxSR0ospGyOy",
    "outputId": "c519d735-e3c2-4ba0-a425-5308c2be1647"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I print the \"econ_valid_encoded\" labels in the set format\n",
    "set(econ_valid_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CiK7sm9pG2q6",
    "outputId": "94d25517-aff6-4f52-c62c-1cc54a5512ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I print the \"sc_valid_encoded\" labels in the set format\n",
    "set(sc_valid_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6xLP4vCHODG"
   },
   "source": [
    "These are the Economic / Non Economic, and Socio-Cultural / Non-Socio-Cultural labels for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8tedj4BHRoj",
    "outputId": "70767363-2aed-471f-92af-144df6591ab2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I print the \"econ_test_encoded\" labels in the set format\n",
    "set(econ_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqFM4RmNHdQA",
    "outputId": "d0fe357c-c6ef-4cb8-b6b7-0a0c30db277c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I print the \"sc_test_encoded\" labels in the set format\n",
    "set(sc_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8VN1naiH5BN"
   },
   "source": [
    "## **3. Creating custom Torch datasets**\n",
    "\n",
    "I now combine the encoded labels and texts into two separate dataset objects. We use the custom Torch `MyDataSet` class to make a `train_dataset` object from the `train_encodings` and `train_encoded` sets for each type of label - i.e., Economic / Socio-Cultural. I also generate a `valid_dataset`, `test_dataset` object from the `test_encodings` and `valid_encodings`, and `valid_encoded` and `test_encoded`, following the same logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "_N96zlDbIo_L"
   },
   "outputs": [],
   "source": [
    "# The MyDataset custom class uses PyTorch's Dataset class as its parent class. \n",
    "# It takes in tokenized text data and their corresponding integer-encoded labels\n",
    "# as inputs and returns these data points in a format suitable for use in PyTorch models.\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  # I define the __init__ method, which initializes the MyDataset object with\n",
    "  # two attributes: \"encodings\" and \"labels\".\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings # \"encodings\" is a dictionary containing the tokenized text data\n",
    "        self.labels = labels # labels is a list containing the integer-encoded labels\n",
    "\n",
    "  # I create the __getitem__ method, which defines how each data point is returned\n",
    "  # from the dataset. The \"idx\" parameter is used to index into the dataset to\n",
    "  # retrieve a specific data point.\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The method initially generates a dictionary item that contains the tokenized\n",
    "        # text data for the corresponding idx index, along with the integer-encoded\n",
    "        # label for the same index.\n",
    "\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # The torch.tensor() function is used to convert the dictionary values to PyTorch tensors.\n",
    "        # The \"key\" variable contains the keys in the encodings dictionary, and \"val\"\n",
    "        # contains the corresponding values for the current idx index.\n",
    "\n",
    "        # I add the integer-encoded label to the dictionary, and I return the latter as the method's output.\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "  # At last, I generate the \"__len__\" method, which returns the dataset's length as its output.\n",
    "  # The latter is equal to the length of the labels list.\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "vmEa1-wkJOv9"
   },
   "outputs": [],
   "source": [
    "# I now apply the MyDataset custom class to the textual and label encodings\n",
    "# for training, validation, and testing, returning them as custom Torch datasets.\n",
    "\n",
    "# a. Economic / Non-Economic\n",
    "train_dataset_econ = MyDataset(train_encodings, econ_train_encoded)\n",
    "valid_dataset_econ = MyDataset(valid_encodings, econ_valid_encoded)\n",
    "test_dataset_econ = MyDataset(test_encodings, econ_test_encoded)\n",
    "\n",
    "# b. Socio-Cultural / Non-Socio-Cultural\n",
    "train_dataset_sc = MyDataset(train_encodings, sc_train_encoded)\n",
    "valid_dataset_sc = MyDataset(valid_encodings, sc_valid_encoded)\n",
    "test_dataset_sc = MyDataset(test_encodings, sc_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoaKaO8ZKMuP"
   },
   "source": [
    "I inspect the newly created custom Torch datasets to check whether there are any issues. This is a bill summary in the Torch `train_dataset_econ` dataset after encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "zju-U4GnKZFY",
    "outputId": "ed4bfe9a-1eb6-412d-bf48-5f80962b1d67"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] protect the homeland from north korean and iranian ballistic missiles act - states the concern of congress over north korean and iranian long - range ballistic missile technology and the spread of such technology . expresses support for ballistic missile protection of u . s . allies and forward deployed forces but also the belief that this should not come at the expense of u . s . homeland protection . directs the secretary of defense to deploy specified numbers of ground - based interceptor ##s in alaska and california and such number in other locations as determined to'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take the first document in the \"train_dataset_econ\" dataset, and I join its\n",
    "# first 100 tokens by a whitespace to get a sneak peek.\n",
    "\n",
    "\" \".join(train_dataset_econ.encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1qYZT4YKwDB"
   },
   "source": [
    "This is a bill summary in the Torch `train_dataset_sc` dataset after encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "dRGXiKfzKyzy",
    "outputId": "1a3acb08-b17e-4d29-9266-6d2b9f0226fb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] protect the homeland from north korean and iranian ballistic missiles act - states the concern of congress over north korean and iranian long - range ballistic missile technology and the spread of such technology . expresses support for ballistic missile protection of u . s . allies and forward deployed forces but also the belief that this should not come at the expense of u . s . homeland protection . directs the secretary of defense to deploy specified numbers of ground - based interceptor ##s in alaska and california and such number in other locations as determined to'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take the first document in the \"train_dataset_sc\" dataset, and I join its\n",
    "# first 100 tokens by a whitespace to get a sneak peek.\n",
    "\n",
    "\" \".join(train_dataset_sc.encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kplfSEaKOJET"
   },
   "source": [
    "This is a bill summary in the Torch `valid_dataset_econ` dataset after encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "xkOM__7ZOuNn",
    "outputId": "9bc67479-b91f-448b-d610-3e99feff5dbc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS] continuation of useful resources to states act or courts act this bill extends and otherwise rev ##ises funding for programs related to child welfare . specifically , the bill extends funding through f ##y ##20 ##22 for the promoting safe and stable families program ; extends funding through f ##y ##20 ##22 for , and otherwise rev ##ises , the grant program for improving courts ' handling of foster - care and adoption proceedings ; and provides funding for the temporary assistance for needy families ( tan ##f ) con ##ting ##ency fund for f ##y ##20 ##19 and\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take the first document in the \"valid_dataset_econ\" dataset, and I join its\n",
    "# first 100 tokens by a whitespace to get a sneak peek.\n",
    "\n",
    "\" \".join(valid_dataset_econ.encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdmSi_MYOOIZ"
   },
   "source": [
    "This is a bill summary in the Torch `valid_dataset_sc` dataset after encoding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "6kIvPiSpOj4e",
    "outputId": "34424b83-7b48-4c51-9f74-293df8eaeda9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS] continuation of useful resources to states act or courts act this bill extends and otherwise rev ##ises funding for programs related to child welfare . specifically , the bill extends funding through f ##y ##20 ##22 for the promoting safe and stable families program ; extends funding through f ##y ##20 ##22 for , and otherwise rev ##ises , the grant program for improving courts ' handling of foster - care and adoption proceedings ; and provides funding for the temporary assistance for needy families ( tan ##f ) con ##ting ##ency fund for f ##y ##20 ##19 and\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take the first document in the \"valid_dataset_sc\" dataset, and I join its\n",
    "# first 100 tokens by a whitespace to get a sneak peek.\n",
    "\n",
    "\" \".join(valid_dataset_sc.encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnT0DhE-OThw"
   },
   "source": [
    "This is a bill summary in the Torch `test_dataset_econ` dataset after encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "vdRuONhBOcrg",
    "outputId": "13a7f618-b5af-4ad3-c172-f7ac8d0b9d73"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] national security commission artificial intelligence act of 2018 this bill establishes , as an independent commission within the executive branch , the national security commission on artificial intelligence to review the advances in artificial intelligence , related machine learning developments , and associated technologies . such commission shall consider the methods and means necessary to advance the development of artificial intelligence , machine learning , and associated technologies by the united states in order to comprehensive ##ly address national security needs , including economic risk , and any other needs of the department of defense or the common defense'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take the first document in the \"test_dataset_econ\" dataset, and I join its\n",
    "# first 100 tokens by a whitespace to get a sneak peek.\n",
    "\n",
    "\" \".join(test_dataset_econ.encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opcKNqulOWum"
   },
   "source": [
    "This is a bill summary in the Torch `test_dataset_sc` dataset after encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "KlWyRx5jOgsP",
    "outputId": "ce8fdcea-68ad-46c7-925b-441fbd38aa91"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[CLS] national security commission artificial intelligence act of 2018 this bill establishes , as an independent commission within the executive branch , the national security commission on artificial intelligence to review the advances in artificial intelligence , related machine learning developments , and associated technologies . such commission shall consider the methods and means necessary to advance the development of artificial intelligence , machine learning , and associated technologies by the united states in order to comprehensive ##ly address national security needs , including economic risk , and any other needs of the department of defense or the common defense'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I take the first document in the \"test_dataset_sc\" dataset, and I join its\n",
    "# first 100 tokens by a whitespace to get a sneak peek.\n",
    "\n",
    "\" \".join(test_dataset_sc.encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSXxUedmPIng"
   },
   "source": [
    "The custom Torch datasets are appropriately set up. It is time to initialise the `bert-base-uncased` pre-trained model and configure its parameters for fine-tuning.\n",
    "\n",
    "<br>\n",
    "\n",
    "## **4. Initialising and configuring the base BERT pre-trained model**\n",
    "I now load the pre-trained `bert-base-uncased` model and transfer it to the Compute Unified Device Architecture (CUDA) for efficient GPU computation. I repeat the process for each classification task and custom Torch dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMBTo4XvSeOG",
    "outputId": "29b06111-b010-4d41-a811-f482f314dcf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# I load the \"AutoModelForSequenceClassification\" class from the HuggingFace\n",
    "# \"transformers\" library, which is optimal for sequence classification tasks.\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# I initialize my pre-trained base-BERT model with the previously set \"model_name\" variable.\n",
    "\n",
    "# The \"num_labels\" parameter is set to the number of unique labels in the dataset,\n",
    "# which is determined by the length of either the \"id2label_econ\", or \"id2label_sc\"\n",
    "# dictionary. This specifies the model how many output labels there are to predict.\n",
    "\n",
    "# Finally, I indicate the device where the model will be stored during training,\n",
    "# which is a GPU device (\"cuda\"), with the \"to\" method.\n",
    "\n",
    "model_econ = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = len(id2label_econ)).to(device_name)\n",
    "model_sc = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = len(id2label_sc)).to(device_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AO4YNxmhT8j2"
   },
   "source": [
    "I now configure the parameters required for fine-tuning the BERT model. These parameters are crucial for fine-tuning my BERT transformers and are specified in the HuggingFace `TrainingArguments` objects that I subsequently pass to the HuggingFace `Trainer` object. While there are numerous other arguments, I will focus on the fundamental ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_kzUDFkAUlC"
   },
   "source": [
    "\n",
    "    | Parameter                   | Explanation                                                                                                                          \n",
    "    |-----------------------------|--------------------------------------------------------------------------------------------------------------------------------------\n",
    "    | num_train_epochs            | The total number of training epochs. This refers to how many times the entire dataset will be processed. Too many epochs can lead to overfitting.\n",
    "    | per_device_train_batch_size | The batch size per device during training.                                                                                           \n",
    "    | per_device_eval_batch_size  | The batch size for evaluation.                                                                                                      \n",
    "    | warmup_steps                | The number of warmup steps for the learning rate scheduler. A smaller value is recommended for small datasets.                         \n",
    "    | weight_decay                | The strength of weight decay, which reduces the size of weights, similar to regularization.                                          \n",
    "    | output_dir                  | The directory where the fine-tuned model and configuration files will be saved.                                                     \n",
    "    | logging_dir                 | The directory where logs will be stored.                                                                                            \n",
    "    | logging_steps               | How often to print logging output. This enables me to terminate training early if the loss is not decreasing.                        \n",
    "    | evaluation_strategy         | Evaluates while training so that I can monitor accuracy improvements.                                                              \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g34xFClyWTbz"
   },
   "source": [
    "I continue by definining a custom evaluation function that returns the model's accuracy and macro-F1 score, since class imbalance is critical for both classification tasks - i.e., most models exhibit the tendency of artificially inflating the number of positive values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "U9FDAEuiWokn"
   },
   "outputs": [],
   "source": [
    "# I define a custom function that takes in the argument \"eval_pred\", which is a\n",
    "# tuple of the form (\"eval_output\", \"eval_dataset\"), containing the evaluation output\n",
    "# and the evaluation dataset.\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    labels = eval_pred.label_ids # Extracting the ground truth labels from the \"eval_pred\" output\n",
    "    preds = eval_pred.predictions.argmax(-1) # Extracting the predicted labels from the \"eval_pred\" output\n",
    "\n",
    "    # I compute the accuracy score thanks to the pre-defined function from the\n",
    "    # \"scikit-learn\" library. It takes in the labels and predictions as inputs and\n",
    "    # returns the accuracy as a float.\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # I follow the same procedure for calculating the macro F1-score. The \n",
    "    # \"sample_weight\" parameter is additionally set to the \"compute_sample_weight\"\n",
    "    # function from \"scikit-learn\" to calculate the sample weights for each class.\n",
    "    macro_f1 = f1_score(labels, preds, average = \"macro\", sample_weight = compute_sample_weight(\"balanced\", labels))\n",
    "\n",
    "    # I return a dictionary with the computed metrics under meaningful keys\n",
    "    return {\"accuracy\": acc, \"macro_f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAUGFunhXFLm"
   },
   "source": [
    "I choose to optimise my models by maximising the macro F1-score, since class imbalance is critical for both classification tasks. Next, I configure the parameters I desire to utilise when fine-tuning the `bert-base-uncased` transformer by instantiating a `TrainingArguments` object. I make an informed choice regarding all parameters, eventually modifying them if performances are unsatisfactory, or there are strong signs of overfitting.\n",
    "\n",
    "First, since my training dataset is moderately-sized, containing 1320 training instances, starting with 5 epochs seems reasonable in my perspective, as I want to avoid overfitting the training data. Second, I employ the standard batch size of 8 for both training and evaluation. I already tried bigger batch sizes of 16 and 32 for training purposes to potentially increase training efficiency, but I found out that this causes memory problems within the freely available CUDA, and I do not want to pay to win.\n",
    "\n",
    "Third, I set an initial learning rate of 5e-5, the standard for fine-tuning BERT models. Fourth, I try to specify a small number of warmup steps (200), still adequate for my medium-sized training dataset, to allow the optimizer to gradually adjust the original learning rate. Fifth, I set the standard weight decay of 0.01, which hopefully will prevent overfitting. Lastly, I command that the training output is logged every 20 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "1qKDQHdKZNBs"
   },
   "outputs": [],
   "source": [
    "metric_name = \"macro_f1\" # I wish to optimise the macro-F1 performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "4uVg-a_ubVqW"
   },
   "outputs": [],
   "source": [
    "# I instantiate an object of the TrainingArguments class, setting the aforementioned parameters:\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    # Number of training epochs\n",
    "    num_train_epochs = 5,\n",
    "    \n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size = 8,\n",
    "    \n",
    "    # Batch size for evaluation\n",
    "    per_device_eval_batch_size = 8,\n",
    "    \n",
    "    # Learning rate for optimization\n",
    "    learning_rate = 5e-5,\n",
    "    \n",
    "    # Load the best model at the end of training\n",
    "    load_best_model_at_end = True,\n",
    "    \n",
    "    # Metric used for selecting the best model (macro F1-score, in our case)\n",
    "    metric_for_best_model = metric_name,\n",
    "    \n",
    "    # Number of warmup steps for the optimizer\n",
    "    warmup_steps = 200,\n",
    "    \n",
    "    # L2 regularization weight decay\n",
    "    weight_decay = 0.01,\n",
    "    \n",
    "    # Directory to save the fine-tuned model and configuration files\n",
    "    output_dir = './results',\n",
    "    \n",
    "    # Directory to store logs\n",
    "    logging_dir = './logs',\n",
    "    \n",
    "    # Log results every n steps\n",
    "    logging_steps = 20,\n",
    "    \n",
    "    # Strategy for evaluating the model during training\n",
    "    evaluation_strategy = 'steps',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAS4-Vu5fFSk"
   },
   "source": [
    "## **5. Fine-Tuning the base BERT models**\n",
    "\n",
    "I now create two HuggingFace `Trainer` objects using the `TrainingArguments` object that I specified above. I also send my `compute_metrics` custom function to the `Trainer` objects, along with my custom Torch datasets for training and validation purposes. I repeat the fine-tuning procedure for both classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "05sg475veFvA"
   },
   "outputs": [],
   "source": [
    "# a. Economic / Non-Economic\n",
    "\n",
    "trainer_econ = Trainer(\n",
    "    model = model_econ, # The instantiated HuggingFace transformer model to be trained\n",
    "    args = training_args, # The training arguments, which I defined above\n",
    "    train_dataset = train_dataset_econ, # The encoded PyTorch training dataset\n",
    "    eval_dataset = valid_dataset_econ, # The encoded PyTorch validation dataset\n",
    "    compute_metrics = compute_metrics # My custom evaluation function \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "puPdU3mieheW"
   },
   "outputs": [],
   "source": [
    "# b. Socio-Cultural / Non-Socio-Cultural\n",
    "\n",
    "trainer_sc = Trainer(\n",
    "    model = model_sc, # The instantiated HuggingFace transformer model to be trained\n",
    "    args = training_args, # The training arguments, which I defined above\n",
    "    train_dataset = train_dataset_sc, # The encoded PyTorch training dataset\n",
    "    eval_dataset = valid_dataset_sc, # The encoded PyTorch validation dataset\n",
    "    compute_metrics = compute_metrics # My custom evaluation function \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KH8VcVU4ezLM"
   },
   "source": [
    "Time to finally fine-tune the models! I start with gearing base BERT towards solving the Economic / Non-Economic classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UQIXlbCSe_Z_",
    "outputId": "ce336284-8823-4e80-9a15-edbd341ecd12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='825' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [825/825 21:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.667800</td>\n",
       "      <td>0.694737</td>\n",
       "      <td>0.570455</td>\n",
       "      <td>0.339155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.695300</td>\n",
       "      <td>0.643662</td>\n",
       "      <td>0.665909</td>\n",
       "      <td>0.595934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.634800</td>\n",
       "      <td>0.568100</td>\n",
       "      <td>0.684091</td>\n",
       "      <td>0.677864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.637762</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.464922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.641900</td>\n",
       "      <td>0.457820</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.751963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.480200</td>\n",
       "      <td>0.429823</td>\n",
       "      <td>0.811364</td>\n",
       "      <td>0.803792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>0.474170</td>\n",
       "      <td>0.784091</td>\n",
       "      <td>0.779454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>0.443117</td>\n",
       "      <td>0.822727</td>\n",
       "      <td>0.819928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.398000</td>\n",
       "      <td>0.467311</td>\n",
       "      <td>0.804545</td>\n",
       "      <td>0.791838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>0.564004</td>\n",
       "      <td>0.770455</td>\n",
       "      <td>0.776618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.502300</td>\n",
       "      <td>0.425018</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.805993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.439759</td>\n",
       "      <td>0.815909</td>\n",
       "      <td>0.809168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.411826</td>\n",
       "      <td>0.815909</td>\n",
       "      <td>0.809168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.363800</td>\n",
       "      <td>0.528597</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.749032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.428050</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.811738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.465600</td>\n",
       "      <td>0.392456</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.819149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.420311</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.826552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.615120</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.808942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>0.596568</td>\n",
       "      <td>0.804545</td>\n",
       "      <td>0.786870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.756364</td>\n",
       "      <td>0.797727</td>\n",
       "      <td>0.799238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.108600</td>\n",
       "      <td>0.795476</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.819177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.638755</td>\n",
       "      <td>0.829545</td>\n",
       "      <td>0.819489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.498561</td>\n",
       "      <td>0.822727</td>\n",
       "      <td>0.817896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.259900</td>\n",
       "      <td>0.725222</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.808942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.723959</td>\n",
       "      <td>0.809091</td>\n",
       "      <td>0.799611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.832015</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.814619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.089900</td>\n",
       "      <td>0.924090</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.805993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.846045</td>\n",
       "      <td>0.809091</td>\n",
       "      <td>0.806679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.898034</td>\n",
       "      <td>0.790909</td>\n",
       "      <td>0.786073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.921675</td>\n",
       "      <td>0.790909</td>\n",
       "      <td>0.782455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.994326</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.790276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>1.018915</td>\n",
       "      <td>0.802273</td>\n",
       "      <td>0.792230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.987571</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.795361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.979151</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.792978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>1.012109</td>\n",
       "      <td>0.809091</td>\n",
       "      <td>0.800359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>1.053540</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.794577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.073581</td>\n",
       "      <td>0.811364</td>\n",
       "      <td>0.797670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>1.081465</td>\n",
       "      <td>0.809091</td>\n",
       "      <td>0.799611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.097525</td>\n",
       "      <td>0.804545</td>\n",
       "      <td>0.797171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>1.098565</td>\n",
       "      <td>0.804545</td>\n",
       "      <td>0.796438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>1.095109</td>\n",
       "      <td>0.802273</td>\n",
       "      <td>0.794476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=825, training_loss=0.2809313911402767, metrics={'train_runtime': 1276.5936, 'train_samples_per_second': 5.17, 'train_steps_per_second': 0.646, 'total_flos': 1736532965376000.0, 'train_loss': 0.2809313911402767, 'epoch': 5.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. Economic / Non Economic\n",
    "\n",
    "trainer_econ.train() # I instruct the GPU to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6--tuJOvh25X",
    "outputId": "4d6c8dcf-d33d-4a23-afd5-17188b1c3da8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='825' max='825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [825/825 21:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.701700</td>\n",
       "      <td>0.683914</td>\n",
       "      <td>0.547727</td>\n",
       "      <td>0.476663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.664814</td>\n",
       "      <td>0.595455</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.639718</td>\n",
       "      <td>0.652273</td>\n",
       "      <td>0.579971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.604900</td>\n",
       "      <td>0.575246</td>\n",
       "      <td>0.695455</td>\n",
       "      <td>0.569850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.468454</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.780225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.673452</td>\n",
       "      <td>0.718182</td>\n",
       "      <td>0.613320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.541800</td>\n",
       "      <td>0.435697</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.801623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.429200</td>\n",
       "      <td>0.518069</td>\n",
       "      <td>0.768182</td>\n",
       "      <td>0.771067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.790909</td>\n",
       "      <td>0.740406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>0.500015</td>\n",
       "      <td>0.809091</td>\n",
       "      <td>0.773869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.546991</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.796404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.488300</td>\n",
       "      <td>0.405654</td>\n",
       "      <td>0.838636</td>\n",
       "      <td>0.842812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.448900</td>\n",
       "      <td>0.557663</td>\n",
       "      <td>0.756818</td>\n",
       "      <td>0.684875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.463522</td>\n",
       "      <td>0.820455</td>\n",
       "      <td>0.801707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.545848</td>\n",
       "      <td>0.809091</td>\n",
       "      <td>0.780015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.469065</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.824122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.765278</td>\n",
       "      <td>0.788636</td>\n",
       "      <td>0.748468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.571192</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.821066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.663258</td>\n",
       "      <td>0.834091</td>\n",
       "      <td>0.819198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.252700</td>\n",
       "      <td>0.788812</td>\n",
       "      <td>0.813636</td>\n",
       "      <td>0.789518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.593340</td>\n",
       "      <td>0.843182</td>\n",
       "      <td>0.846654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.279500</td>\n",
       "      <td>0.817840</td>\n",
       "      <td>0.788636</td>\n",
       "      <td>0.741524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.632166</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.799851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.735147</td>\n",
       "      <td>0.811364</td>\n",
       "      <td>0.786535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.407700</td>\n",
       "      <td>0.750290</td>\n",
       "      <td>0.804545</td>\n",
       "      <td>0.768968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.652348</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.835604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.183800</td>\n",
       "      <td>0.721745</td>\n",
       "      <td>0.834091</td>\n",
       "      <td>0.810635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.705163</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.816857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.678887</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.815780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.637513</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.829872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>0.637978</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.844189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>0.736084</td>\n",
       "      <td>0.822727</td>\n",
       "      <td>0.800274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.830974</td>\n",
       "      <td>0.804545</td>\n",
       "      <td>0.773948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.802498</td>\n",
       "      <td>0.815909</td>\n",
       "      <td>0.791360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>0.753300</td>\n",
       "      <td>0.838636</td>\n",
       "      <td>0.824984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.730754</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.830945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.153700</td>\n",
       "      <td>0.745048</td>\n",
       "      <td>0.838636</td>\n",
       "      <td>0.827993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.767256</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.829872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.781319</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.827097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.793743</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.828875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.794556</td>\n",
       "      <td>0.843182</td>\n",
       "      <td>0.828728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=825, training_loss=0.30044037621129643, metrics={'train_runtime': 1262.9639, 'train_samples_per_second': 5.226, 'train_steps_per_second': 0.653, 'total_flos': 1736532965376000.0, 'train_loss': 0.30044037621129643, 'epoch': 5.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. Socio-Cultural / Non-Socio-Cultural\n",
    "\n",
    "trainer_sc.train() # I instruct the GPU to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxAbG1_fflus"
   },
   "source": [
    "## **6. Evaluating and Testing the Fine-Tuned Models**\n",
    "\n",
    "After having trained the models, I wish to validate and test them on the respective sets. I call the `.evaluate` method of the `Trainer` object, which automatically runs the built-in validation procedure, referring to my custom `compute_metrics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "IgR3BEZpwJYT",
    "outputId": "702c33f0-bfd3-4f7a-d261-8cf6109a6728"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7239589095115662,\n",
       " 'eval_accuracy': 0.8090909090909091,\n",
       " 'eval_macro_f1': 0.7996110237700574,\n",
       " 'eval_runtime': 15.12,\n",
       " 'eval_samples_per_second': 29.101,\n",
       " 'eval_steps_per_second': 3.638,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a. Economic / Non-Economic\n",
    "\n",
    "trainer_econ.evaluate() # I instruct the GPU to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "R44Q3V66fzLn",
    "outputId": "29ddf00f-4596-4080-8263-c8048f98ca12"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7502904534339905,\n",
       " 'eval_accuracy': 0.8045454545454546,\n",
       " 'eval_macro_f1': 0.7689675314266936,\n",
       " 'eval_runtime': 15.13,\n",
       " 'eval_samples_per_second': 29.081,\n",
       " 'eval_steps_per_second': 3.635,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b. Socio-Cultural / Non-Socio-Cultural\n",
    "\n",
    "trainer_sc.evaluate() # I instruct the GPU to evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpEnxR8-EfaG"
   },
   "source": [
    "`LEGAL-BERT` appears to fare better than the model I fine-tuned with the more generic base BERT in the Economic / Non-Economic classification task, as its evaluation loss is much lower (0.488 versus 0.724), and both its accuracy (0.82) and macro-F1 score (0.82) are all higher than the base BERT's corresponding metrics - i.e., 0.81, and 0.80.\n",
    "\n",
    "On the other hand, `LEGAL-BERT`'s performance with the Socio-Cultural / Non-Socio-Cultural classification is comparable to the base BERT's one. The latter model's evaluation loss is lower - i.e., 0.750 versus 0.855 - but the accuracies and macro-F1 scores convey a slightly different message, with the former model being seemingly, yet only slightly better. Both its accuracy (0.81) and macro-F1 score (0.78) are marginally higher than base BERT's corresponding metrics - i.e., 0.80, and 0.77.\n",
    "\n",
    "To corroborate these findings, I wish to get a more detailed evaluation of the fine-tuned models, with precision and recall metrics for all categories, and to account for potential overfitting of the validation set. Hence, I extract the labels predicted from the test set and compare them with the \"ground truth\" labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ROyDKPh9E5xx",
    "outputId": "e031595a-7b63-4a8c-ef5d-03bf46595051"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a. Economic / Non-Economic\n",
    "\n",
    "# I call the predict() method to make the predictions on the PyTorch test set\n",
    "predicted_results_econ = trainer_econ.predict(test_dataset_econ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRCaVQ-vFKs-",
    "outputId": "d3e279ce-2b78-4cd8-a312-23d1c605f349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"predicted_results_econ\" object is a 2D matrix with all the predicted\n",
    "# probabilities for the respective output labels, for each document contained in\n",
    "# the test set (440 in total).\n",
    "\n",
    "predicted_results_econ.predictions.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "S44qWOtAKrW5"
   },
   "outputs": [],
   "source": [
    "# I get the prediction with the highest probability\n",
    "predicted_labels_econ = predicted_results_econ.predictions.argmax(-1)\n",
    "\n",
    "# I flatten the predictions into a one-dimensional list object\n",
    "predicted_labels_econ = predicted_labels_econ.flatten().tolist()\n",
    "\n",
    "# I convert from integers back to strings for readability with my custom dictionary\n",
    "predicted_labels_econ = [id2label_econ[lab] for lab in predicted_labels_econ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0y6chIyvLBlC",
    "outputId": "0b7bcaf7-d994-43aa-d0a6-293c3a8671cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Economic       0.79      0.87      0.83       222\n",
      "Non-Economic       0.85      0.77      0.81       218\n",
      "\n",
      "    accuracy                           0.82       440\n",
      "   macro avg       0.82      0.82      0.82       440\n",
      "weighted avg       0.82      0.82      0.82       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I compare the predictions against the \"ground truth\" labels with \"scikit-learn\"\n",
    "print(classification_report(econ_test, predicted_labels_econ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgg_nNsMLI0W"
   },
   "source": [
    "The test's results corroborate the validation step's findings, although they must be taken with added caution since the test sample is almost perfectly balanced, whereas the overall sample is skewed towards the positive label. Nevertheless, the fine-tuned base BERT achieves a performance that is comparable to `LEGAL-BERT`'s with an identical 0.82 accuracy. However, its precisions and recalls are not as solid across the board, and exhibit a marginal tendency of yielding an inflated number of positive labels, while `LEGAL-BERT`'s are never lower than 0.80 - i.e., extremely solid all across the board. This means that only `LEGAL-BERT` is capable of classifying Economic / Non-Economic documents without yielding an inflated number of positive labels, unlike base BERT. Thus, I keep the fine-tuned `LEGAL-BERT` as my final choice for drawing my Economic / Non-Economic predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "niA1DBwaLQUw",
    "outputId": "6e7c2704-df2d-49e8-a0d2-80bf95a14fb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# b. Socio-Cultural / Non-Socio-Cultural\n",
    "\n",
    "# I call the predict() method to make the predictions on the PyTorch test set\n",
    "predicted_results_sc = trainer_sc.predict(test_dataset_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKtUUwEwLc3A",
    "outputId": "d659d7ae-0a34-4661-b166-038fe323be72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"predicted_results_sc\" object is a 2D matrix with all the predicted\n",
    "# probabilities for the respective output labels, for each document contained in\n",
    "# the test set (440 in total).\n",
    "\n",
    "predicted_results_sc.predictions.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "qSLBj10gLnZi"
   },
   "outputs": [],
   "source": [
    "# I get the prediction with the highest probability\n",
    "predicted_labels_sc = predicted_results_sc.predictions.argmax(-1)\n",
    "\n",
    "# I flatten the predictions into a one-dimensional list object\n",
    "predicted_labels_sc = predicted_labels_sc.flatten().tolist()\n",
    "\n",
    "# I convert from integers back to strings for readability with my custom dictionary\n",
    "predicted_labels_sc = [id2label_sc[lab] for lab in predicted_labels_sc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6v9I9G6vMMqz",
    "outputId": "05fdb349-2f4a-4fc9-ca30-0595effce31c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Non-Socio-Cultural       0.95      0.73      0.82       157\n",
      "    Socio-Cultural       0.87      0.98      0.92       283\n",
      "\n",
      "          accuracy                           0.89       440\n",
      "         macro avg       0.91      0.85      0.87       440\n",
      "      weighted avg       0.90      0.89      0.88       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I compare the predictions against the \"ground truth\" labels with \"scikit-learn\"\n",
    "print(classification_report(sc_test, predicted_labels_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWKzOtayMQVn"
   },
   "source": [
    "On the other hand, this final test's results change my interpretation of the validation step's findings, which were way less promising for base BERT. Base BERT shows higher accuracy than `LEGAL-BERT` - i.e., a whopping 0.89, versus `LEGAL-BERT`'s 0.87. While performance is partially driven by the amount of positive labels predicted by the model, this seems to be less of an issue than in `LEGAL-BERT`'s case. This is proven by base BERT's higher precisions and recalls all across the board, Thus, I keep the fine-tuned base-BERT as my final choice for drawing my Socio-Cultural / Non-Socio-Cultural predictions.\n",
    "\n",
    "## **7. Wrapping Up**\n",
    "On a final note, I must specify that I wished to run an explicit hyperparameter tuning script for the few `Trainer` object hyperparameters that could be improved - i.e, the number of training epochs (3, 5, or 7), the learning rate (1e-5, 5e-5, or 1e-4), and the number of warmup steps (100, 200, or 300).\n",
    "\n",
    "This could be done by creating a `ParameterGrid` object with the appropriate `sklearn` function, iterating over each hyperparameter combination within this object, and updating the `training_args` object by calling the `transformers` `update_from_dict` method within the loop. The model would be re-initialised at each iteration, so to ensure that each training iteration starts from an identical initial state, independently of the training executed during previous iterations. Another option would be looking into the `raytune` library, for achieving state-of-the-art hyperparameter tuning.\n",
    "\n",
    "However, the GPU from Google CoLab is giving me serious problems, and constantly kicks me out or does not let me connect to the backend. Furthermore, I continously run into memory / RAM allocation issues when trying more complex procedures - i.e., changing batch sizes to improve the training's efficiency - or pre-trained models. Therefore, after partially experimenting (and failing!) in the background, I deem this to be the best solution I can come up with in the current conditions. It is unlikely that trying combinations for three parameters only could have changed the results by much, so I believe this approximation is acceptable in my research's context.\n",
    "\n",
    "To conclude, I save both models and their configuration files to their set directories within Google CoLab, to keep them for making my predictions on a separate script, with the `save_model` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "log2RVwgMk01"
   },
   "outputs": [],
   "source": [
    "# a. Economic / Non-Economic\n",
    "\n",
    "trainer_econ.save_model(save_directory_econ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "DZPFo0rXMld5"
   },
   "outputs": [],
   "source": [
    "# b Socio-Cultural / Non-Socio-Cultural\n",
    "\n",
    "trainer_sc.save_model(save_directory_sc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
